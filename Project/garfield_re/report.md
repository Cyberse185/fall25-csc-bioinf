# Abstract

Variant calling in next-generation sequencing generates high false-positive rates that require filtering. GARFIELD-NGS (Genomic vARiant FIltering by dEep Learning moDels) uses deep learning to improve variant filtering accuracy over traditional methods. We successfully replicated the published GARFIELD-NGS results, achieving AUC scores within 0.3% of reported values. Additionally, we sought to provide external validation of the authors' claim that GARFIELD-NGS is more robust at lower sequencing coverage by generating matched 30× downsampled datasets from three NA12878 whole-genome samples and re-calling variants with GATK.

# Introduction

A significant challenge in Next-Generation Sequencing (NGS) pipelines is the generation of false positive variant calls due to sequencing artifacts and alignment errors. While the Genome Analysis Toolkit (GATK) established a foundational framework for addressing these challenges (McKenna et al., 2010), accurately filtering artifacts remains difficult despite subsequent research efforts.

GARFIELD-NGS (Ravasio et al., 2018) proposes a solution by applying Deep Neural Networks (DNNs) to variant annotations generated by GATK. The authors claim that deep learning can better capture the complex, non-linear interactions between feature metrics, offering superior robustness compared to traditional baseline classifiers. 

In this project, we evaluate these claims through two objectives. First, we replicate the original GARFIELD-NGS models using the H2O deep learning framework to verify that our reproduced AUC scores match the published results. Second, we perform an external robustness analysis by generating 30× downsampled datasets from three NA12878 whole-genome samples, re-calling variants with GATK, and quantifying model performance degradation under reduced coverage.

# Methods

Our focus consists of two parts: (1) direct replication of published GARFIELD-NGS results using the original datasets, and (2) evaluation of model performance on independently generated variant calls at different coverage depths. 

Model performance was quantified using the AUC metric, and all models were implemented using the H2O.ai framework in Python (H2O.ai, 2015).

## Replication Datasets

For the replication phase, we obtained pre-processed training, validation, and test datasets from the official GARFIELD-NGS GitHub repository. These datasets were derived from NA12878 Illumina exome sequencing data. Ground truth labels (True/False Positive) were established by the original authors by intersecting variant calls with the Genome in a Bottle (GIAB) v3.3.2 high-confidence regions (Zook et al., 2014).

## Feature Selection

**Illumina Features:** The model utilizes 10 standard GATK annotations: BaseQRankSum, ReadPosRankSum, MQ, MQRankSum, FS, SOR , QD , DP , QUAL, and GQ.

**Ion Torrent Features:** For Ion Torrent data, distinct flow-based metrics from the Torrent Variant Caller (TVC) are used. The SNV model incorporates 18 features: FDP, FAO, QD, FSAF, FSAR, FXX, LEN, HRUN, RBI, VARB, STB, STBP, PB, PBP, MLLD, SSSB, QUAL, and GQ. The INDEL model uses a nearly identical set, substituting PB and PBP with SSEN and SSEP.

## Baseline Classifiers

Following the authors' methodology, we implemented two baseline classifiers using H2O:

- **Random Forest:** 500 trees with maximum depth of 50 and early stopping (5 rounds) based on log-loss with a tolerance of $10^{-2}$.
- **Logistic Regression:** Implemented using the H2OGeneralizedLinearEstimator with default parameters.

## Deep Learning Protocol

To replicate the original study's training dynamics, we stuck to their specific protocols. Instead of using H2O's automated class balancing, we applied a manual downsampling strategy to the pre-training and training datasets by randomly removing true positive calls to ensure a minimum false positive prevalence of 20%. The validation and test sets were left unbalanced to reflect real-world artifact ratios.

Training proceeded in two stages. First, network weights were initialized using an unsupervised deep autoencoder trained for 1,000 epochs (stopping metric: MSE) on the balanced pre-training data. These weights were then transferred to a supervised Deep Learning Estimator for fine-tuning. The final supervised training utilized a mini-batch size of 1 and ran for up to 1,000 epochs with early stopping based on log-loss (tolerance $10^{-3}$). To ensure reproducibility, fixed random seeds were applied to both the H2O cluster and the Python environment.

## Final Model Architectures

| Parameter | Illumina INDELs | Illumina SNVs | Ion INDELs | Ion SNVs |
|:----------|:----------------|:--------------|:-----------|:---------|
| Activation | Rectifier (ReLU) | Tanh | Tanh* | Tanh |
| Hidden Layers | [10, 70, 60, 10, 10] | [20, 60, 60, 50, 70] | [100, 40, 50, 90, 90] | [40, 90, 90, 30, 100] |
| Rho | 0.961 | 0.956 | 0.985 | 0.978 |
| Epsilon | 1.0E-09 | 1.0E-10 | 1.0E-09 | 1.0E-10 |
| L1 Regularization | 0.00015 | 0.0022 | 0.00026 | 0.0128 |
| L2 Regularization | 0.01311 | 0.00293 | 0.05488 | 0.00224 |
| Mini-batch Size | 1 | 1 | 1 | 1 |

*\*See Discussion section for activation function substitution details.*

## Robustness Analysis Method

To validate model robustness at lower coverage, we processed three NA12878 whole-genome sequencing runs (SRR098401, SRR1611180, SRR3197786). Reads were aligned to hg19 using BWA-MEM (Li, 2013) and deduplicated with GATK MarkDuplicates. To simulate low-coverage conditions, we used `samtools view -s` (Li et al., 2009) to downsample BAM files to a target depth of 30×, generating matched "Full Coverage" and "30× Coverage" datasets for all three replicates.

Variants were called using GATK HaplotypeCaller and labeled against the Genome in a Bottle (GIAB) v3.3.2 truth set; variants matching GIAB calls in both position and allele were labeled as true positives. We aggregated the data across all three samples, imputed missing annotation values with 0, and performed a 50% random split to generate the final test sets. The pre-trained GARFIELD-NGS, Random Forest, and Logistic Regression models were then evaluated on both coverage conditions.

# Results

## 1. Replication of Published Results

We successfully replicated the performance of the original GARFIELD-NGS models. As shown in Table 1, our Illumina models matched the reported AUCs with high precision (differences of +0.002 to +0.003). The Ion Torrent models showed slight deviations (see Limitations and Key Observations section).

**Table 1: GARFIELD-NGS Replication vs. Published Results**

| Dataset | Platform | Variant Type | Paper Test Set AUC | Our Test Set AUC | Difference |
|:--------|:---------|:-------------|:-------------------|:-----------------|:-----------|
| Illumina SNV | Illumina | SNV | 0.7998 | 0.8021 | +0.0023 |
| Illumina INDEL | Illumina | INDEL | 0.9269 | 0.9299 | +0.0030 |
| Ion SNV | Ion Torrent | SNV | 0.9757 | 0.9596 | -0.0161 |
| Ion INDEL | Ion Torrent | INDEL | 0.9464 | 0.9242 | -0.0222 |

## 2. Robustness to Coverage Reduction

To validate model utility in lower-coverage sequencing scenarios, we evaluated performance on independently generated 30× downsampled datasets (Illumina SNVs only).

**Table 2: Model Robustness to Coverage Reduction**

*95% confidence intervals reported for AUC values.*

| Model | Full Coverage AUC | 30× Coverage AUC | Absolute Change | Relative Change (%) |
|:------|:-----------------:|:----------------:|:---------------:|:-------------------:|
| GARFIELD-NGS | 0.7790 ± 0.0010 | 0.7754 ± 0.0017 | -0.0036 | -0.5% |
| Logistic Regression | 0.7499 ± 0.0011 | 0.7344 ± 0.0018 | -0.0154 | -2.1% |
| Random Forest | 0.7474 ± 0.0011 | 0.7598 ± 0.0017 | +0.0124* | +1.7% |

*\*See below for the RF discussion.*

# Discussion

Our results validate the primary claim of the GARFIELD-NGS authors: deep learning models exhibit superior stability against coverage degradation compared to baseline classifiers. While the Logistic Regression model suffered a 2.1% performance drop at 30× coverage, GARFIELD-NGS maintained nearly identical performance (0.5% drop, within confidence intervals), suggesting it has learned feature representations robust to depth-dependent variations.

## Limitations and Key Observations

**Sample Size:** Due to computational constraints, our external validation was limited to 3 sequencing runs (N=3) compared to the larger cohorts (N=23) used in the original study. Additionally, we focused exclusively on SNVs for the robustness analysis.

**Random Forest:** Table 2 reveals a counter-intuitive result where Random Forest performance improved at 30× coverage. This could potentially be due to the prior variant caller selection bias. GATK HaplotypeCaller might have failed to call many low-confidence variants that are called at full coverage, and so the 30× dataset would contain fewer hard-to-classify edge cases, benefitting the tree-based model. 

**Numerical Stability (ReLU vs. Tanh):** The original paper specifies Rectifier (ReLU) activation for Ion INDELs. However, we encountered numerical instability (exploding gradients) during replication with ReLU. We substituted Tanh activation, which stabilized training but likely contributed to the lower replication AUC observed for the Ion INDEL model (-0.0222 difference).

**Reproducibility:** H2O's default implementation uses "Hogwild!" stochastic gradient descent (Niu et al., 2011), which is non-deterministic. To achieve reproducible results, we explicitly enabled the `reproducible=True` flag and set fixed random seeds—a critical detail not emphasized in the original documentation.

# Acknowledgements

Large Language Models (Claude 4.5 Sonnet and Gemini 3.0 Pro) were utilized for code implementation and debugging throughout this project.

# References

H2O.ai. (2015). *H2O: Scalable Machine Learning*. Retrieved from http://www.h2o.ai

Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., Marth, G., Abecasis, G., Durbin, R., & 1000 Genome Project Data Processing Subgroup. (2009). The Sequence Alignment/Map format and SAMtools. *Bioinformatics*, 25(16), 2078–2079.

Li, H. (2013). Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM. *arXiv preprint arXiv:1303.3997*.

McKenna, A., Hanna, M., Banks, E., Sivachenko, A., Cibulskis, K., Kernytsky, A., Garimella, K., Altshuler, D., Gabriel, S., Daly, M., & DePristo, M. A. (2010). The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data. *Genome Research*, 20(9), 1297–1303.

Niu, F., Recht, B., Ré, C., & Wright, S. J. (2011). Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. *Advances in Neural Information Processing Systems* (NIPS).

Ravasio, V., et al. (2018). GARFIELD-NGS: Genomic vARiant FIltering by dEep Learning moDels. *PLOS ONE*.

Zook, J. M., et al. (2014). Integrating human sequence data sets provides a resource of benchmark SNP and indel genotype calls. *Nature Biotechnology*, 32(3), 246–251.
